{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ðŸ§  MNIST & Fashion-MNIST Classification Project\n",
        "---\n",
        "\n",
        "##### [**Author**] : Aditya Saxena\n",
        "\n",
        "This project explores both classic and modern techniques for image classification using two datasets:\n",
        "\n",
        "- **MNIST**: 70,000 grayscale images of handwritten digits (0â€“9)\n",
        "- **Fashion-MNIST**: 70,000 grayscale images of fashion items (10 categories)\n",
        "\n",
        "We implement and evaluate:\n",
        "- Linear classifiers from scratch: Perceptron, Average Perceptron, Pegasos (SVM)\n",
        "- Deep learning models using Keras: Dense Neural Networks (DNN) and Convolutional Neural Networks (CNN)\n",
        "- Evaluation using hinge loss, accuracy, and hyperparameter tuning\n",
        "\n",
        "All experiments are performed using **Python** and **Jupyter Notebooks** for clarity, interactivity, and reproducibility.\n"
      ],
      "metadata": {
        "id": "MzQktcLntYdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Defining Utility Functions\n",
        "---"
      ],
      "metadata": {
        "id": "x8CSyTy1wNFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ `zero_one_loss` â€“ General Classification Loss\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Measures how many predictions differ from true labels.\n",
        "\n",
        "**Formula:**  \n",
        "For \\( n \\) samples:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}[\\hat{y}_i \\ne y_i]\n",
        "$$\n",
        "\n",
        "**Parameters:**  \n",
        "- `predictions`: 1D NumPy array of predicted labels  \n",
        "- `labels`: 1D NumPy array of true labels  \n",
        "\n",
        "**Returns:**  \n",
        "- Average misclassification rate (float between 0 and 1)\n"
      ],
      "metadata": {
        "id": "BV7NXluNChuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_one_loss(predictions, labels):\n",
        "    \"\"\"\n",
        "    Computes the proportion of incorrect predictions.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: 1D NumPy array of predicted class labels\n",
        "    - labels: 1D NumPy array of true class labels\n",
        "\n",
        "    Returns:\n",
        "    - Zero-One Loss: float in [0, 1]\n",
        "    \"\"\"\n",
        "    # Count mismatches between predictions and true labels\n",
        "    errors = predictions != labels\n",
        "\n",
        "    # Compute mean error rate (number of mismatches / total)\n",
        "    return np.mean(errors)\n"
      ],
      "metadata": {
        "id": "9jLjEYC7CiLp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Hinge Loss (Single Example)\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Measures margin-based classification error for one example; used in SVMs.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\max(0,\\ 1 - y(\\theta^\\top x + \\theta_0))\n",
        "$$\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_vector (x)`: input features  \n",
        "- `label (y)`: true class label (Â±1)  \n",
        "- `theta`: weight vector  \n",
        "- `theta_0`: bias term\n"
      ],
      "metadata": {
        "id": "PTE2vgZAwRpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hinge_loss_single(feature_vector, label, theta, theta_0):\n",
        "    # Compute the raw model output (a linear combination of weights and input)\n",
        "    y = theta @ feature_vector + theta_0  # Equivalent to np.dot(theta, feature_vector) + theta_0\n",
        "\n",
        "    # Calculate the hinge loss:\n",
        "    # If the prediction is correct and confidently outside the margin (i.e., y * label â‰¥ 1), loss is 0.\n",
        "    # Otherwise, loss increases as the prediction moves closer to or past the margin.\n",
        "    return max(0, 1 - y * label)\n"
      ],
      "metadata": {
        "id": "h7BqWrwSwMYn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Hinge Loss (Full Dataset)\n",
        "---\n",
        "**Purpose:**  \n",
        "Computes the **average hinge loss** over all training samples.\n",
        "\n",
        "**Formula:**  \n",
        "\\[\n",
        "\\text{Average Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\max(0,\\ 1 - y_i(\\theta^\\top x_i + \\theta_0))\n",
        "\\]\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_matrix`: matrix of input features (shape: n Ã— d)  \n",
        "- `labels`: true class labels (Â±1)  \n",
        "- `theta`: weight vector  \n",
        "- `theta_0`: bias term"
      ],
      "metadata": {
        "id": "adJxXnCAyDZA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YbSQirsUswj2"
      },
      "outputs": [],
      "source": [
        "def hinge_loss_full(feature_matrix, labels, theta, theta_0):\n",
        "    # Initialize total loss to accumulate individual hinge losses\n",
        "    total_loss = 0\n",
        "\n",
        "    # Get the number of data points (rows) in the dataset\n",
        "    n_samples = feature_matrix.shape[0]\n",
        "\n",
        "    # Loop through each example and compute the hinge loss\n",
        "    for i in range(n_samples):\n",
        "        # Add the hinge loss for the i-th sample to the total\n",
        "        total_loss += hinge_loss_single(feature_matrix[i], labels[i], theta, theta_0)\n",
        "\n",
        "    # Return the average hinge loss over all samples\n",
        "    return total_loss / n_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Perceptron Single Step Update\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Performs a single update of the Perceptron algorithm if the current prediction is incorrect or on the decision boundary.\n",
        "\n",
        "**Formula:**  \n",
        "\n",
        "If  \n",
        "\n",
        "$$\n",
        "y(\\theta^\\top x + \\theta_0) \\leq 0\n",
        "$$\n",
        "\n",
        "then update:  \n",
        "\n",
        "$$\n",
        "\\theta := \\theta + yx,\\quad \\theta_0 := \\theta_0 + y\n",
        "$$\n",
        "\n",
        "Otherwise, no change.\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_vector (x)`: input feature vector  \n",
        "- `label (y)`: true class label (Â±1)  \n",
        "- `current_theta`: current weight vector  \n",
        "- `current_theta_0`: current bias term"
      ],
      "metadata": {
        "id": "dszIJzEczHHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_single_step_update(feature_vector, label, current_theta, current_theta_0):\n",
        "    # Compute the prediction score (dot product + bias)\n",
        "    prediction = np.dot(current_theta, feature_vector) + current_theta_0\n",
        "\n",
        "    # If the prediction is incorrect or on the boundary (y * score <= 0)\n",
        "    # We use a small epsilon (1e-7) to handle floating point precision issues\n",
        "    if label * prediction <= 1e-7:\n",
        "        # Update weights and bias in the direction of the true label\n",
        "        updated_theta = current_theta + label * feature_vector\n",
        "        updated_theta_0 = current_theta_0 + label\n",
        "        return updated_theta, updated_theta_0\n",
        "\n",
        "    # If the prediction is correct and confidently classified, no update needed\n",
        "    return current_theta, current_theta_0"
      ],
      "metadata": {
        "id": "WCgJyHF4zStv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Perceptron (Full Algorithm)\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Trains a linear classifier by iteratively updating weights using the Perceptron learning rule over multiple passes through the dataset.\n",
        "\n",
        "**Formula:**  \n",
        "For each misclassified example \\((x_i, y_i)\\), if:\n",
        "\n",
        "$$\n",
        "y_i(\\theta^\\top x_i + \\theta_0) \\leq 0\n",
        "$$\n",
        "\n",
        "Then update:\n",
        "\n",
        "$$\n",
        "\\theta := \\theta + y_i x_i,\\quad \\theta_0 := \\theta_0 + y_i\n",
        "$$\n",
        "\n",
        "Repeat for \\( T \\) full passes over the training data.\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_matrix`: NumPy matrix (n Ã— d), where each row is a data point  \n",
        "- `labels`: NumPy array of length \\( n \\) with true class labels (Â±1)  \n",
        "- `T`: number of iterations (epochs) over the full dataset\n"
      ],
      "metadata": {
        "id": "rLujo6H8z-il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(feature_matrix, labels, T):\n",
        "    \"\"\"\n",
        "    Full Perceptron training loop.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_matrix: 2D NumPy array where each row is a feature vector\n",
        "    - labels: 1D NumPy array of labels (Â±1)\n",
        "    - T: number of full passes through the data (epochs)\n",
        "\n",
        "    Returns:\n",
        "    - theta: learned weight vector\n",
        "    - theta_0: learned bias term\n",
        "    \"\"\"\n",
        "    (nsamples, nfeatures) = feature_matrix.shape\n",
        "\n",
        "    # Initialize weights and bias to zero\n",
        "    theta = np.zeros(nfeatures)\n",
        "    theta_0 = 0.0\n",
        "\n",
        "    # Repeat for T full iterations over the dataset\n",
        "    for t in range(T):\n",
        "        # get_order provides a shuffled index order each pass\n",
        "        for i in get_order(nsamples):\n",
        "            x_i = feature_matrix[i]\n",
        "            y_i = labels[i]\n",
        "\n",
        "            # Apply single-step Perceptron update\n",
        "            theta, theta_0 = perceptron_single_step_update(x_i, y_i, theta, theta_0)\n",
        "\n",
        "    return theta, theta_0\n"
      ],
      "metadata": {
        "id": "4qZob26xz_8x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Pegasos Single Step Update\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Performs one update step of the Pegasos algorithm for Support Vector Machines (SVM), using stochastic gradient descent and hinge loss.\n",
        "\n",
        "**Formula:**  \n",
        "If margin is violated:  \n",
        "$$\n",
        "y(\\theta^\\top x + \\theta_0) \\leq 1\n",
        "$$  \n",
        "then update:  \n",
        "$$\n",
        "\\theta := \\theta + \\eta(yx - \\lambda\\theta),\\quad \\theta_0 := \\theta_0 + \\eta y\n",
        "$$\n",
        "\n",
        "Else (no violation):  \n",
        "$$\n",
        "\\theta := \\theta - \\eta \\lambda \\theta,\\quad \\theta_0 := \\theta_0\n",
        "$$\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_vector (x)`: single input sample  \n",
        "- `label (y)`: true class label (Â±1)  \n",
        "- `L`: regularization parameter (Î»)  \n",
        "- `eta`: learning rate  \n",
        "- `theta`: current weight vector  \n",
        "- `theta_0`: current bias term\n"
      ],
      "metadata": {
        "id": "EhDzmBaR0wtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pegasos_single_step_update(feature_vector, label, L, eta, theta, theta_0):\n",
        "    \"\"\"\n",
        "    One step of the Pegasos update rule for binary SVM.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_vector: 1D NumPy array for a single data point\n",
        "    - label: True class label (Â±1)\n",
        "    - L: Regularization parameter (lambda)\n",
        "    - eta: Learning rate\n",
        "    - theta: Current weight vector\n",
        "    - theta_0: Current bias term\n",
        "\n",
        "    Returns:\n",
        "    - Updated (theta, theta_0)\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute margin: how confidently the point is classified\n",
        "    margin_factor = label * (np.dot(feature_vector, theta) + theta_0)\n",
        "\n",
        "    # Check for hinge loss violation (i.e., margin â‰¤ 1)\n",
        "    is_violation = 1.0 if margin_factor <= 1 else 0.0\n",
        "\n",
        "    # Update rule:\n",
        "    # - Move in direction of label * feature_vector if violating margin\n",
        "    # - Always apply L2 regularization by shrinking theta\n",
        "    new_theta = theta + eta * (is_violation * label * feature_vector - L * theta)\n",
        "    new_theta_0 = theta_0 + eta * (is_violation * label * 1.0)\n",
        "\n",
        "    return new_theta, new_theta_0\n"
      ],
      "metadata": {
        "id": "Cb4PW8kM02jv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Pegasos (Full Algorithm)\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Trains a binary Support Vector Machine (SVM) using the Pegasos algorithm (a stochastic sub-gradient descent method) with L2 regularization.\n",
        "\n",
        "**Formula:**  \n",
        "For each training example \\((x_i, y_i)\\) and update count \\( t \\), use:\n",
        "\n",
        "- Learning rate:  \n",
        "$$\n",
        "\\eta = \\frac{1}{\\sqrt{t}}\n",
        "$$\n",
        "\n",
        "- If margin violated:  \n",
        "$$\n",
        "y_i(\\theta^\\top x_i + \\theta_0) \\leq 1\n",
        "$$  \n",
        "Then update:  \n",
        "$$\n",
        "\\theta := \\theta + \\eta (y_i x_i - \\lambda \\theta),\\quad \\theta_0 := \\theta_0 + \\eta y_i\n",
        "$$\n",
        "\n",
        "Else:  \n",
        "$$\n",
        "\\theta := \\theta - \\eta \\lambda \\theta,\\quad \\theta_0 := \\theta_0\n",
        "$$\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_matrix`: NumPy matrix of shape (n, d)  \n",
        "- `labels`: NumPy array of true labels (Â±1)  \n",
        "- `T`: number of full iterations over the data  \n",
        "- `L`: regularization parameter (Î»)\n"
      ],
      "metadata": {
        "id": "kt-jcHa71N7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pegasos(feature_matrix, labels, T, L):\n",
        "    \"\"\"\n",
        "    Full Pegasos training loop.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_matrix: 2D NumPy array (each row is a data point)\n",
        "    - labels: 1D NumPy array of Â±1 class labels\n",
        "    - T: number of full passes through the data (epochs)\n",
        "    - L: regularization parameter (lambda)\n",
        "\n",
        "    Returns:\n",
        "    - theta: learned weight vector\n",
        "    - theta_0: learned bias term\n",
        "    \"\"\"\n",
        "\n",
        "    (nsamples, nfeatures) = feature_matrix.shape\n",
        "\n",
        "    # Initialize weights and bias\n",
        "    theta = np.zeros(nfeatures)\n",
        "    theta_0 = 0.0\n",
        "\n",
        "    # Counter for total updates (used for learning rate scheduling)\n",
        "    count = 0\n",
        "\n",
        "    # Loop over T epochs\n",
        "    for t in range(T):\n",
        "        # Visit each sample in a shuffled order\n",
        "        for i in get_order(nsamples):\n",
        "            count += 1\n",
        "            eta = 1.0 / np.sqrt(count)  # Dynamic learning rate\n",
        "\n",
        "            # Perform single Pegasos update step\n",
        "            theta, theta_0 = pegasos_single_step_update(\n",
        "                feature_matrix[i], labels[i], L, eta, theta, theta_0\n",
        "            )\n",
        "\n",
        "    return theta, theta_0\n"
      ],
      "metadata": {
        "id": "HAJDpLAW1U__"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ `classify` â€“ Linear Prediction Rule\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Predicts labels using a linear classifier.\n",
        "\n",
        "**Formula:**  \n",
        "$$\n",
        "\\hat{y} = \\text{sign}(\\theta^\\top x + \\theta_0)\n",
        "$$\n",
        "\n",
        "**Parameters:**  \n",
        "- `feature_matrix`: 2D NumPy array (n_samples Ã— n_features)  \n",
        "- `theta`: weight vector  \n",
        "- `theta_0`: bias term  \n",
        "\n",
        "**Returns:**  \n",
        "- 1D NumPy array of predicted labels (+1 or -1)\n"
      ],
      "metadata": {
        "id": "APuVkE8i3m91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(feature_matrix, theta, theta_0):\n",
        "    \"\"\"\n",
        "    Predicts labels using a linear decision rule.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_matrix: 2D array of shape (n_samples, n_features)\n",
        "    - theta: Weight vector from trained model\n",
        "    - theta_0: Bias term from trained model\n",
        "\n",
        "    Returns:\n",
        "    - Array of predictions: +1 or -1\n",
        "    \"\"\"\n",
        "    # Compute raw scores (linear combination + bias)\n",
        "    scores = np.dot(feature_matrix, theta) + theta_0\n",
        "\n",
        "    # Apply sign rule: +1 if score > 0, else -1\n",
        "    return np.where(scores > 0, 1, -1)\n"
      ],
      "metadata": {
        "id": "40SUAgDO3ryt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ `classification_accuracy` â€“ Accuracy Metric\n",
        "\n",
        "**Purpose:**  \n",
        "Calculates the proportion of correct predictions made by a classifier. It is the most direct and interpretable evaluation metric in classification tasks.\n",
        "\n",
        "**Formula:**  \n",
        "$$\n",
        "\\text{Accuracy} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}[\\hat{y}_i = y_i]\n",
        "$$\n",
        "\n",
        "**Parameters:**  \n",
        "- `predictions`: 1D NumPy array of predicted class labels  \n",
        "- `labels`: 1D NumPy array of ground truth labels  \n",
        "\n",
        "**Returns:**  \n",
        "- Float value between 0 and 1 indicating classification accuracy\n"
      ],
      "metadata": {
        "id": "H8Rbt7Sh3thB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_accuracy(predictions, labels):\n",
        "    \"\"\"\n",
        "    Computes classification accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: 1D NumPy array of predicted labels\n",
        "    - labels: 1D NumPy array of true labels\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: Float (proportion of correct predictions)\n",
        "    \"\"\"\n",
        "    return np.mean(predictions == labels)\n"
      ],
      "metadata": {
        "id": "ioPfkUPp3tyd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ `plot_metrics_over_epochs` â€“ Line Plot of Accuracy or Loss\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Visualizes the change in a performance metric (e.g., accuracy or loss) over training epochs for both train and test sets.\n",
        "\n",
        "**Parameters:**  \n",
        "- `train_metrics`: list of metric values per epoch (e.g., train accuracy)  \n",
        "- `test_metrics`: list of metric values per epoch (e.g., test accuracy)  \n",
        "- `metric_name`: string to label the y-axis (e.g., \"Accuracy\" or \"Loss\")  \n",
        "- `model_name`: name of the model, displayed in title and legend  \n",
        "\n",
        "**Returns:**  \n",
        "- A matplotlib plot showing the metric progression over epochs.\n"
      ],
      "metadata": {
        "id": "GDR_LKD2C8-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics_over_epochs(train_metrics, test_metrics, metric_name=\"Accuracy\", model_name=\"\"):\n",
        "    \"\"\"\n",
        "    Plots a given metric over epochs for train and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - train_metrics: list of metric values for training set (1 per epoch)\n",
        "    - test_metrics: list of metric values for test set (1 per epoch)\n",
        "    - metric_name: label for the y-axis (e.g., \"Accuracy\", \"Loss\")\n",
        "    - model_name: name of the model being plotted\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(train_metrics) + 1))\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, train_metrics, label=f\"Train {metric_name}\")\n",
        "    plt.plot(epochs, test_metrics, label=f\"Test {metric_name}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title(f\"{metric_name} Over Epochs â€“ {model_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lbX-zpDhC8wq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ `report_results_table` â€“ Tabular Summary of Model Results\n",
        "---\n",
        "\n",
        "**Purpose:**  \n",
        "Displays a clean table comparing multiple models on evaluation metrics.\n",
        "\n",
        "**Parameters:**  \n",
        "- `results_dict`: a dictionary where each key is a model name, and the value is another dictionary with:\n",
        "  - \"Train Accuracy\"\n",
        "  - \"Test Accuracy\"\n",
        "  - \"Train Loss\"\n",
        "  - \"Test Loss\"\n",
        "\n",
        "**Returns:**  \n",
        "- A styled and formatted table (Pandas DataFrame) rendered in the notebook.\n"
      ],
      "metadata": {
        "id": "5py7qkONC8ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def report_results_table(results_dict):\n",
        "    \"\"\"\n",
        "    Generates a styled summary table of model performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - results_dict: Dictionary with model names as keys and a dictionary of\n",
        "                    metrics as values. Example:\n",
        "                    {\n",
        "                      \"Perceptron\": {\"Train Accuracy\": 0.9, \"Test Accuracy\": 0.85, ...},\n",
        "                      \"Pegasos\": {\"Train Accuracy\": 0.95, \"Test Accuracy\": 0.91, ...}\n",
        "                    }\n",
        "\n",
        "    Returns:\n",
        "    - Styled DataFrame shown using display()\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
        "    styled_df = df.style.format(\"{:.4f}\").set_caption(\"ðŸ“‹ Model Performance Summary\")\n",
        "    display(styled_df)\n"
      ],
      "metadata": {
        "id": "t216msMFC8NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oCq5wH9uC73k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LpEORGolC7jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Re6O2BdiC7FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [ Solution ] Solving Using Linear Classifiers Only"
      ],
      "metadata": {
        "id": "Ne4qhuVh1idE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to be continued"
      ],
      "metadata": {
        "id": "nPVdJIkL1zkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}